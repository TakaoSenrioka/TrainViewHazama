import os
import csv
import time
import subprocess
import sys
import select
import requests
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re

# å®šæ•°å®šç¾©
PUBLIC_DIR = "public"
CHIBA_FILE = os.path.join(PUBLIC_DIR, "chiba_timetable.csv")
CUSTOM_FILE = os.path.join(PUBLIC_DIR, "custom.csv")
ROUTES_FILE = "routes.csv"
KEIO_GROUP = ["äº¬ç‹ç·š", "äº¬ç‹ç›¸æ¨¡åŸç·š", "äº¬ç‹é«˜å°¾ç·š"]
CHIBA_UPDATE_INTERVAL = 60  # seconds
ROUTE_UPDATE_INTERVAL = 180  # seconds

os.makedirs(PUBLIC_DIR, exist_ok=True)

# ãƒ—ãƒ­ã‚°ãƒ©ãƒ 1: åƒè‘‰æ™‚åˆ»è¡¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
startid = "00180695"  # è»Šå‚ä¸Š
goalid = "00180769"   # åƒè‘‰é§…
chiba_url = f'https://transfer.navitime.biz/keiseibus/pc/location/BusLocationResult?startId={startid}&goalId={goalid}'

def extract_time(text, prefix):
    m = re.search(rf'{prefix}(\d{{2}}:\d{{2}})', text)
    return m.group(1) if m else ""

def extract_minutes_info(text):
    m = re.search(r'(\d+)åˆ†å¾Œã«åˆ°ç€', text)
    return m.group(1) if m else ""

def subtract_minutes(time_str, minutes):
    try:
        t = datetime.strptime(time_str, '%H:%M')
        t -= timedelta(minutes=minutes)
        return t.strftime('%H:%M')
    except:
        return "00:00"

def scrape_prediction_times(url):
    response = requests.get(url)
    response.encoding = 'utf-8'
    soup = BeautifulSoup(response.text, 'html.parser')
    result = []
    for item in soup.select('li.plotList'):
        course_name_elem = item.select_one('.courseName')
        prediction_times = item.select('.predictionTime')
        last_leave_time, last_minutes_info, last_delay_time = None, None, None
        for pt in prediction_times:
            text = pt.text.strip().replace('\n', '').replace('\t', '')
            parent = pt.parent
            if "å®šåˆ»" in text:
                minutes_info = ""
                if parent:
                    for s in parent.stripped_strings:
                        if "åˆ†å¾Œã«åˆ°ç€" in s:
                            minutes_info = extract_minutes_info(s.strip())
                            break
                m = re.search(r'é…ã‚Œ(\d+)åˆ†', text)
                delay_time = m.group(1) if m else ""
                leave_time = extract_time(text, "å®šåˆ»:")
                last_leave_time, last_minutes_info, last_delay_time = leave_time, minutes_info, delay_time
            elif "åˆ°ç€äºˆå®š" in text:
                arrive_time = extract_time(text, "åˆ°ç€äºˆå®š:")
                if arrive_time:
                    leave_time = last_leave_time or subtract_minutes(arrive_time, 19)
                    delay_time = last_delay_time if last_leave_time else "0"
                    minutes_info = last_minutes_info if last_leave_time else "0"
                    result.append({
                        "leave_time": leave_time,
                        "delay_time": delay_time,
                        "minutes_info": minutes_info
                    })
                    last_leave_time = last_minutes_info = last_delay_time = None
    return result if result else [{"leave_time": "00:00", "delay_time": "0", "minutes_info": "0"}]

def save_chiba_csv():
    scraped_data = scrape_prediction_times(chiba_url)
    with open(CHIBA_FILE, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['leave_time', 'delay_time', 'minutes_info'])
        for row in scraped_data:
            writer.writerow([row['leave_time'], row['delay_time'], row['minutes_info']])
    print(f"ğŸšŒ åƒè‘‰ãƒ‡ãƒ¼ã‚¿ã‚’ {CHIBA_FILE} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    git_push("chiba_timetable.csv æ›´æ–°")

# ãƒ—ãƒ­ã‚°ãƒ©ãƒ 2: è·¯ç·šé‹è¡ŒçŠ¶æ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
def scrape_routes():
    df = pd.read_csv(ROUTES_FILE)
    keio_results, other_results = [], []
    for _, row in df.iterrows():
        line_name, url = row["è·¯ç·šå"], row["URL"]
        try:
            response = requests.get(url, timeout=10)
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, "html.parser")
            suspend = soup.find("dd", class_="trouble suspend")
            trouble = None if suspend else soup.find("dd", class_="trouble")
            info_text = suspend.get_text(strip=True) if suspend else trouble.get_text(strip=True) if trouble else "å¹³å¸¸é‹è»¢"
            if "è¦‹åˆã‚ã›" in info_text:
                status = "é‹è»¢è¦‹åˆã‚ã›"
            elif "é…ã‚Œ" in info_text or "ãƒ€ã‚¤ãƒ¤ãŒä¹±ã‚Œ" in info_text:
                status = "é…å»¶"
            elif "é‹ä¼‘" in info_text:
                status = "é‹ä¼‘"
            elif "ç›´é€šé‹è»¢ã‚’ä¸­æ­¢" in info_text:
                status = "ç›´é€šé‹è»¢ä¸­æ­¢"
            elif info_text == "å¹³å¸¸é‹è»¢":
                continue
            else:
                status = "æƒ…å ±"
            entry = {"è·¯ç·šå": line_name, "é‹è¡Œæƒ…å ±": info_text, "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": status}
            (keio_results if line_name in KEIO_GROUP else other_results).append(entry)
        except Exception as e:
            print(f"âŒ {line_name} ã‚¨ãƒ©ãƒ¼: {e}")

    results = keio_results or other_results or [{
        "è·¯ç·šå": "ç¾åœ¨ã®é‹è¡ŒçŠ¶æ³ï¼š",
        "é‹è¡Œæƒ…å ±": f"é¦–éƒ½åœã®é‰„é“è·¯ç·šã¯ãŠãŠã‚€ã­å¹³å¸¸é‹è»¢ã§ã™ã€‚ï¼ˆ{time.strftime('%-mæœˆ%-dæ—¥%Hæ™‚%Måˆ†')}æ›´æ–°ï¼‰",
        "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": "å¹³å¸¸é‹è»¢",
    }]

    output_path = os.path.join(PUBLIC_DIR, "result.csv")
    pd.DataFrame(results).to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"ğŸšƒ è·¯ç·šãƒ‡ãƒ¼ã‚¿ã‚’ {output_path} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    return output_path

def wait_and_accept_input():
    print("âœ‰ï¸ 2åˆ†å¾…æ©Ÿä¸­ã€‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚Œã°å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆEnterã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰ï¼š")
    print("ğŸ‘‰ å…¥åŠ› > ", end='', flush=True)
    ready, _, _ = select.select([sys.stdin], [], [], 120)
    if ready:
        user_input = sys.stdin.readline().strip()
        if user_input:
            with open(CUSTOM_FILE, "w", encoding="utf-8") as f:
                f.write(user_input + "\n")
            print(f"ğŸ“¥ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ {CUSTOM_FILE} ã«æ›¸ãè¾¼ã¿ã¾ã—ãŸã€‚")
        else:
            print("ğŸ“¤ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã—ã€‚")
    else:
        print("âŒ› ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

def git_push(message="Auto update"):
    status = subprocess.run(["git", "status", "--porcelain"], capture_output=True, text=True)
    if status.stdout.strip():
        try:
            subprocess.run(["git", "add", "."], check=True)
            subprocess.run(["git", "commit", "-m", message], check=True)
            subprocess.run(["git", "push"], check=True)
            print("ğŸš€ GitHub ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¾ã—ãŸã€‚")
        except subprocess.CalledProcessError as e:
            print(f"âš ï¸ Gitæ“ä½œã‚¨ãƒ©ãƒ¼: {e}")
    else:
        print("ğŸŸ¢ å¤‰æ›´ãªã—ã€‚GitHubã¸ã®ãƒ—ãƒƒã‚·ãƒ¥ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

# ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
if __name__ == "__main__":
    last_route_time = 0
    while True:
        now = time.time()
        # ãƒ—ãƒ­ã‚°ãƒ©ãƒ 1ã®å‡¦ç†ï¼ˆæ¯åˆ†å®Ÿè¡Œï¼‰
        save_chiba_csv()

        # ãƒ—ãƒ­ã‚°ãƒ©ãƒ 2ã®å‡¦ç†ï¼ˆ3åˆ†ã”ã¨ï¼‰
        if now - last_route_time >= ROUTE_UPDATE_INTERVAL:
            result_path = scrape_routes()
            wait_and_accept_input()
            git_push("é‹è¡Œæƒ…å ±ã‚’æ›´æ–°")
            last_route_time = now

        # æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã¾ã§1ç§’å¾…æ©Ÿ
        time.sleep(CHIBA_UPDATE_INTERVAL)
